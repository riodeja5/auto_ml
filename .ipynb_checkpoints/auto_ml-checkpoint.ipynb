{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC # SVCは非線形SVM（Support Vector Classifier）\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# ファイル入出力ライブラリ\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoML():\n",
    "    def __init__(self, categorical_columns, eval_kind):\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.eval_kind = eval_kind\n",
    "\n",
    "        # 2値分類ができるモデルをパイプラインで作成\n",
    "        self.pipelines = {\n",
    "            'knn':\n",
    "                Pipeline([('scl',StandardScaler()), ('est',KNeighborsClassifier())]),\n",
    "            'logistic':\n",
    "                Pipeline([('scl',StandardScaler()), ('est',LogisticRegression(random_state=1))]),\n",
    "            'rsvc':\n",
    "                Pipeline([('scl',StandardScaler()), ('est',SVC(C=1.0,kernel='rbf',class_weight='balanced',random_state=1, probability=True))]),\n",
    "#             'lsvc':\n",
    "#                 Pipeline([('scl',StandardScaler()), ('est',LinearSVC(C=1.0,class_weight='balanced',random_state=1))]),\n",
    "            'rf':\n",
    "                Pipeline([('scl',StandardScaler()), ('est',RandomForestClassifier(random_state=1))]),\n",
    "            'gb':\n",
    "                Pipeline([('scl',StandardScaler()), ('est',GradientBoostingClassifier(random_state=1))]),\n",
    "            'mlp':\n",
    "                Pipeline([('scl',StandardScaler()), ('est',MLPClassifier(hidden_layer_sizes=(5,3), max_iter=500, random_state=1))])\n",
    "        }\n",
    "        \n",
    "    def read_data_file(self, file_path):\n",
    "        # objectで読み込むのがポイント！ 'Dependents_2'が'Dependents_2.0'となることを避けられる\n",
    "        dtype = {column: object for column in self.categorical_columns}\n",
    "        df = pd.read_csv(file_path, header=0, dtype=dtype)\n",
    "\n",
    "        X  = df.iloc[:,2:]            # 3列目以降を特徴量\n",
    "        ID = df.iloc[:,[0]]             # 第0列はPK（Loan_ID）なのでIDとしてセット\n",
    "        y  = df.iloc[:,1]              # 2列目をクラス変数\n",
    "        \n",
    "        self.ID_name = ID.columns[0]\n",
    "        self.y_name = y.name\n",
    "        \n",
    "        return X, y, ID\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # データ前処理\n",
    "        X_pre = self.__preprocess_X(X)\n",
    "        y_pre = self.__preprocess_y(y)\n",
    "        \n",
    "        # one-hotエンコーディングヘッダをテストデータにも適用するため保持\n",
    "        self.X_columns = X_pre.columns.values\n",
    "#         \n",
    "#         # デバッグ文\n",
    "#         print('---X_columns start------------')\n",
    "#         display(self.X_columns)\n",
    "#         print('---X_columns end------------')\n",
    "        \n",
    "        # 全てのモデルでfitを行う（cross-validationで検証する）\n",
    "        # fit & evaluation\n",
    "        scores = {}\n",
    "        for pipe_name, pipeline in self.pipelines.items():\n",
    "            pipeline.fit(X_pre, y_pre)\n",
    "            cv_results = cross_val_score(pipeline,\n",
    "                             X_pre,\n",
    "                             y_pre.values,\n",
    "                             cv=5,\n",
    "                             scoring=self.eval_kind)\n",
    "\n",
    "            # cross-validationの平均値-標準偏差をスコアとする\n",
    "            scores[pipe_name] = cv_results.mean() - cv_results.std()\n",
    "            sorted_scores = self.__sort_dictionary(scores)\n",
    "\n",
    "        # アルゴリズムランキングと性能指標評価を出力\n",
    "        display(pd.Series(sorted_scores))\n",
    "\n",
    "        # eval_kindを用いて評価する\n",
    "        # 一番評価の良かったモデルを取っておく\n",
    "        best_algorithm = [*sorted_scores][0]\n",
    "#         print('best:{}'.format(best_algorithm))\n",
    "        self.best_model = self.pipelines[best_algorithm]\n",
    "    \n",
    "    # X前処理\n",
    "    def __preprocess_X(self, X):\n",
    "        # Xのone-hotエンコーディング\n",
    "        X_ohe = pd.get_dummies(X, dummy_na=True, columns=self.categorical_columns)\n",
    "\n",
    "        # 欠損値を平均で置き換える\n",
    "        imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "        imp.fit(X_ohe)\n",
    "        X_ohe_columns = X_ohe.columns.values\n",
    "        X_ohe = pd.DataFrame(imp.transform(X_ohe), columns=X_ohe_columns)\n",
    "        \n",
    "        # デバッグ文\n",
    "        print('preprocess result-----')\n",
    "        display(X_ohe)\n",
    "        print('----------------------')\n",
    "        \n",
    "        return X_ohe\n",
    "\n",
    "    # y前処理\n",
    "    def __preprocess_y(self, y):\n",
    "        # yは欠損値を最頻値で置き換える\n",
    "        y_pre = y.fillna(y.mode()[0])\n",
    "#         # デバッグ文\n",
    "#         print('preprocess result-----')\n",
    "#         display(y_pre)\n",
    "#         print('----------------------')\n",
    "        return y_pre\n",
    "    \n",
    "    def __sort_dictionary(self, dict):\n",
    "        sorted_dict = {}\n",
    "        for k, v in sorted(dict.items(), key=lambda x: -x[1]):\n",
    "            sorted_dict[k] = v\n",
    "        \n",
    "        return sorted_dict\n",
    "    \n",
    "    def __get_score(self, y, y_predict):\n",
    "        if self.eval_kind == 'accuracy':\n",
    "            return accuracy_score(y, y_predict)\n",
    "        if self.eval_kind == 'precision':\n",
    "            return precision_score(y, y_predict)\n",
    "        if self.eval_kind == 'recall':\n",
    "            return recall_score(y, y_predict)\n",
    "        if self.eval_kind == 'f1':\n",
    "            return f1_score(y, y_predict)\n",
    "\n",
    "    def __preprocess_test(self, X_test):\n",
    "        # データ前処理\n",
    "        X_test_pre = self.__preprocess_X(X_test)\n",
    "\n",
    "        # 訓練データのヘッダから空データフレーム作成\n",
    "        df_cols_m = pd.DataFrame(None,\n",
    "                         columns=self.X_columns,\n",
    "                         dtype=float)\n",
    "#         # デバッグ文\n",
    "#         print('---df_cols_m start--------------------')\n",
    "#         display(df_cols_m)\n",
    "#         print('---df_cols_m end----------------------')\n",
    "        \n",
    "        # テストデータの列を訓練データに合わせる\n",
    "        X_test_concat = pd.concat([df_cols_m, X_test_pre])\n",
    "\n",
    "#         # デバッグ文\n",
    "#         display(X_test_concat)\n",
    "        \n",
    "        # 訓練データにない列を削除\n",
    "        X_test_drop = X_test_concat.drop(list(set(X_test_concat.columns.values)-set(self.X_columns)),axis=1)\n",
    "        \n",
    "\n",
    "        # テストデータに登場しなかったデータ項目をゼロ埋め\n",
    "        X_test_drop.loc[:,list(set(self.X_columns)-set(X_test_pre.columns.values))] = \\\n",
    "            X_test_drop.loc[:,list(set(self.X_columns)-set(X_test_pre.columns.values))].fillna(0, axis=1)\n",
    "\n",
    "#         # デバッグ文\n",
    "#         print('---X_test_drop start--------------------')\n",
    "#         display(X_test_drop)\n",
    "#         print('---X_test_drop end----------------------')\n",
    "\n",
    "        # 訓練データと合わせて並び替え\n",
    "        X_test_drop_reindex = X_test_drop.reindex(self.X_columns, axis=1)\n",
    "\n",
    "        # デバッグ文\n",
    "        print('---X_test_drop_reindex start--------------------')\n",
    "        display(X_test_drop_reindex)\n",
    "        print('---X_test_drop_reindex end----------------------')\n",
    "\n",
    "        return X_test_drop_reindex\n",
    "        \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # データ前処理\n",
    "        X_test_pre_complete = self.__preprocess_test(X_test)\n",
    "        \n",
    "        return self.best_model.predict(X_test_pre_complete)\n",
    "                \n",
    "    def predict_proba(self, X_test):\n",
    "        # データ前処理\n",
    "        X_test_pre_complete = self.__preprocess_test(X_test)\n",
    "\n",
    "        return self.best_model.predict_proba(X_test_pre_complete)\n",
    "\n",
    "    def predict_proba_with_id(self, id, X_test):\n",
    "        proba = self.predict_proba(X_test)\n",
    "        \n",
    "        # 2列目が'1'の予測確率\n",
    "        proba_1 = proba[:, 1]\n",
    "        \n",
    "        # IDと予測確率の結合\n",
    "        ID_array = ID_test.values\n",
    "        ID_array_1dim = ID_array[:,0]\n",
    "        result = np.vstack((ID_array_1dim, proba_1))\n",
    "        result_df = pd.DataFrame(result).T\n",
    "        \n",
    "        # ヘッダをつける\n",
    "        result_df.columns = [self.ID_name, self.y_name]\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    # file_nameにAutoMLごとベストモデルを保存する\n",
    "    def save(self, file_name):\n",
    "        with open(file_name, mode='wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    # file_nameから学習済みモデルを保持したAutoMLを呼び出す\n",
    "    def load(self, file_name):\n",
    "        with open(file_name, mode='rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto MLの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリカル変数\n",
    "categorical_columns = ['sales',\n",
    "                       'salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ml = AutoML(categorical_columns, eval_kind='accuracy')\n",
    "# auto_ml = AutoML(categorical_columns, eval_kind='precision')\n",
    "# auto_ml = AutoML(categorical_columns, eval_kind='recall')\n",
    "# auto_ml = AutoML(categorical_columns, eval_kind='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本番データ\n",
    "X, y, ID = auto_ml.read_data_file('data/final_hr_analysis_train.csv')\n",
    "\n",
    "# 欠損値\n",
    "# X, y, ID = auto_ml.read_data_file('data/final_hr_analysis_train_lack.csv')\n",
    "\n",
    "# クラス変数欠損\n",
    "# X, y, ID = auto_ml.read_data_file('data/final_hr_analysis_train_class_lack.csv')\n",
    "\n",
    "# カテゴリデータがTestと異なる\n",
    "# X, y, ID = auto_ml.read_data_file('data/final_hr_analysis_train_category.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model生成（fit）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_model = auto_ml.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テストデータ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本番データ\n",
    "X_test, y_test, ID_test = auto_ml.read_data_file('data/final_hr_analysis_test.csv')\n",
    "# 欠損値\n",
    "# X_test, y_test, ID_test = auto_ml.read_data_file('data/final_hr_analysis_test_lack.csv')\n",
    "# カテゴリデータが訓練データと異なる\n",
    "# X_test, y_test, ID_test = auto_ml.read_data_file('data/final_hr_analysis_test_category.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 予測（predict）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto_ml.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 予測確率（predict_proba）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proba = auto_ml.predict_proba_with_id(X_test=X_test, id=ID_test)\n",
    "# proba_train = auto_ml.predict_proba_with_id(X_test=X, id=ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proba\n",
    "# proba_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba.to_csv('aijc1303.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ml.save('best_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習済みモデル（保存したモデル）を呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = auto_ml.load('best_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スコア用データに対し予測確率を付与できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model.predict_proba_with_id(id=ID_test, X_test=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
